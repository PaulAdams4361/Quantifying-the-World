---
title: "Real-Time Indoor Location Positioning"
author: "Justin Howard, Paul Adams, and Stuart Miller"
date: September 8, 2020
lang: en-US
class: man
# figsintext: true
numbersections: true
encoding: UTF-8
bibliography: references.bib
biblio-style: apalike
output:
  bookdown::pdf_document2:
     citation_package: natbib
     keep_tex: true
     toc: false
header-includes:
   - \usepackage{amsmath}
   - \usepackage{gensymb}
   - \usepackage[utf8]{inputenc}
   - \usepackage[T1]{fontenc}
   - \usepackage{setspace}
   - \usepackage{hyperref}
   - \onehalfspacing
   - \newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
editor_options: 
  chunk_output_type: console
---

```{r, lib-read, results='hide', message=FALSE, include=FALSE, echo=FALSE}
# libraries
library(pacman)
p_load(tidyverse, caret, dplyr, magrittr, knitr, tidyr)
#library(tidyverse)
#library(caret)
#library(dplyr)
#library(magrittr)
#library(knitr)
#library(tidyr)
setwd('C:/Users/Pablo/Desktop/qtw-repo_2/case_study_1')
source('./functions.R')

# set a random seed for repodicibility
set.seed(42)

# load the data
offline <- readData()
online <- readData(filename = './online.data.txt',
                   subMacs = unique(offline$mac))

macAddresses <- sort(unique(offline$mac))

# create a unique position identifier
offline$posXY <- paste(offline$posX, offline$posY, sep = '-')
online$posXY <- paste(online$posX, online$posY, sep = '-')
```

# Introduction


Real Time Location Systems (RTLS) that are capable of tracking business assets and people in special circumstances,
 are a popular area of research. 
Warehouse distribution and delivery services have grown more rapidly due to the COVID-19 pandemic,
 and with that growth, we can expect the relative importance of tracking assets at scale to increase.
In this case study, we assess whether it is possible to predict locations based on the measurement
 of WiFi signals from fixed access points (AP)\footnotemark.
Additionally, two APs are placed in close proximity, we assess the impact of using both of these APs.
The location sensing system analyzed in this case study is located in an indoor office environment
 on a single floor of a building.

\footnotetext{This analysis was conducted with the R programming language (\cite{R}).}

# Methods

## Data 

### Data Collection

The data was collected in an indoor office environment on a single floor with 7 fixed WiFi APs.
The WiFi APs are identifiable by their media-access-control (MAC) addresses.
It is shown by \cite{NolanLang} that two of these APs 
 (\texttt{00:0f:a3:39:e1:c0} and \texttt{00:0f:a3:39:dd:cd})
 are positioned in close proximity.
The experiment area was split into a grid 166 measurement points, which were spaced 1 meter apart.
The points of measurement are referred to as `posX` and `posY`.
Using a handheld WiFi scanning device, WiFi signal strength measurements were collected at each of the 166 locations.
Additionally, the handheld WiFi scanning device was rotated to 8 angles 
 (45\degree  increments from 0)
 at each measurement location.
The resulting dataset contains approximately one million measurements.

### Data Cleaning

The data are provided as measurement logs with the MAC address of the scanning device, 
 measurement position, measurement orientation, AP MAC address, AP signal strength,
 and timestamp on a single line.
We extracted the data by reading each line and tokenizing each unit of information.
The data was initially organized in a long format data frame with
 time, position X, Y, and Z, orientation, AP MAC address, signal strength, and channel as the columns.
Once the data was organized, we were able to determine that several columns were not necessary.
Specifically, we removed position Z and the scan MAC address because these were fixed throughout the experiment
 and channel because each AP used a single channel throughout the experiment.
We also found that several MAC addresses were coded as \textit{adhoc} (\texttt{type=1}).
These were removed since we are only interested in positioning based on the fixed APs (\texttt{type=3}).
Additionally, we determined that there some noise present in the orientation measurement.
The experiment specified that only 8 angles were considered (each 45\degree  increment from 0).
All orientation values were rouned to the nearest 45\degree increment.
Any missing signal values were filled with -100, imputing it as a very low strength signal.

 
```{r, pivotal chunk, echo=FALSE}
offline_pivot <- offline %>% 
   tidyr::pivot_wider(names_from = mac,
               values_from = 'signal',
               values_fn = list(signal=mean))
offline_pivot[is.na(offline_pivot)] <- -100

online_pivot <- online %>% 
   tidyr::pivot_wider(names_from = mac,
               values_from = 'signal',
               values_fn = list(signal=mean))
online_pivot[is.na(online_pivot)] <- -100
```


## Modeling Method

We chose to model the position as a function of the WiFi singal strength with 
\textit{k}-nearest neighbors (\textit{k}-NN).
Intuitively, a \textit{k}-NN model will predict a new position as the average positions of the closest \textit{k}
 data points in the training data based on the WiFi signals.
Since the features are microwaves, which can pass through physical barriers,
 we used euclidean distance to determine the closest signals.
To use a \textit{k}-NN, the value of k most be estimated based on the training data.
We used 5-fold cross validation to determine the value of \textit{k},
 selecting the value of \textit{k} that produced the lowest error based on root meean squared error (RMSE).

An advantage of \textit{k}-NN is that it is non-parametric, thus it not necessary to assess whether the data
 exhibit certain distributional properties.
However, a potential disadvantage of \textit{k}-NN is the computational complexity,
 which is given by (\ref{eqn:knncomplexity}) for large $N$.
This time complexity means a \textit{k}-NN would not be suitable for modeling position in large areas as the number measurement points would be large.


\begin{equation}
\label{eqn:knncomplexity}
O\left(n\right) \sim \frac{n^2}{2}
\end{equation}



## Model Validation

Predicting the real-time X- and Y-coordinate positions using K-Nearest Neighbors required the constructing an ensemble model. The first
model we built contained all devices. The second model we built included all but the device excluding MAC address 00:0f:a3:39:dd:cd.
Finally, we developed a third model which did not include the device identified as 00:0f:a3:39:e1:c0. We made this comparison because
both of those explicitly mentioned devices were positioned at the same location within the study. We modeled along this process for
both X- and Y-coordinate location prediction sets.

In order to simulate a measure of error that could be expected after applying to the data once the system is online (online data set), we applied cross-validation to a holdout data set, comprised of 25% of the offline data set. For this analysis, we trained the offline
data set on 75%.



```{r, very very important chunk of learning, echo=FALSE}

offline_subset <- select(offline_pivot, c("posX", "posY", 'posXY', "angle",
                                    "00:14:bf:b1:97:8a", "00:14:bf:b1:97:90", "00:0f:a3:39:e1:c0",
                                    "00:14:bf:b1:97:8d", "00:14:bf:b1:97:81", "00:14:bf:3b:c7:c6",
                                    "00:0f:a3:39:dd:cd"))

rows <- sample(nrow(offline_subset))
offline_shuffed_subset <- offline_subset[rows,]

offline_train_75 = offline_shuffed_subset[1:floor((dim(offline_shuffed_subset)[[1]]*0.75)),] #686,213 training samples
offline_test_25 = offline_shuffed_subset[ceiling((dim(offline_shuffed_subset)[[1]]*0.75)):dim(offline_shuffed_subset)[[1]],] # 228, 738 testing samples

offline_train_subset <- offline_train_75 %>% filter(angle < 135-4 & angle < 135+4)
offline_test_subset <- offline_test_25 %>% filter(angle < 135-4 & angle < 135+4)

#offline_subset <- offline_subset %>% filter(angle > 135 - 4 & angle < 135 + 4)


knn.grid <- expand.grid(k = seq(1:15))

train.control <- trainControl(method = 'cv', number = 5)

# fit a model for posX with AP c0
model.c0.x <- train(
   posX ~ . ,
   data = offline_train_subset %>% select(-c("posY","posXY", "00:0f:a3:39:dd:cd")),
   method = 'knn',
   trControl = train.control,
   tuneGrid = knn.grid
)

# fit a model for posY with AP c0
model.c0.y <- train(
   posY ~ . ,
   data = offline_train_subset %>% select(-c("posX","posXY", "00:0f:a3:39:dd:cd")),
   method = 'knn',
   trControl = train.control,
   tuneGrid = knn.grid
)

# fit a model for posX with AP cd
model.cd.x <- train(
   posX ~ . ,
   data = offline_train_subset %>% select(-c("posY","posXY", "00:0f:a3:39:e1:c0")),
   method = 'knn',
   trControl = train.control,
   tuneGrid = knn.grid
)

# fit a model for posY with AP cd
model.cd.y <- train(
   posY ~ . ,
   data = offline_train_subset %>% select(-c("posX","posXY", "00:0f:a3:39:e1:c0")),
   method = 'knn',
   trControl = train.control,
   tuneGrid = knn.grid
)

# fit a model for posX with all APs
model.all.x <- train(
   posX ~ . ,
   data = offline_train_subset %>% select(-c("posY","posXY")),
   method = 'knn',
   trControl = train.control,
   tuneGrid = knn.grid
)

# fit a model for posY with all APs
model.all.y <- train(
   posY ~ . ,
   data = offline_train_subset %>% select(-c("posX","posXY")),
   method = 'knn',
   trControl = train.control,
   tuneGrid = knn.grid
)


```


```{r, Hold-Out Testing for Online Error Simulation - Preds and RMSE, results='hide', message=FALSE, include=FALSE, echo=FALSE}
X_preds.allMAC <- predict(model.all.x, newdata = subset(offline_test_subset, select = -c(`posY`, `posXY`)))
Y_preds.allMAC <- predict(model.all.y, newdata = subset(offline_test_subset, select = -c(`posX`, `posXY`)))

X_res = data.frame(X_preds.allMAC, offline_test_subset$posX)
Y_res = data.frame(Y_preds.allMAC, offline_test_subset$posY)
names(X_res) <- c('preds','actuals')
names(Y_res) <- c('preds','actuals')
X_res$residuals = X_res$preds - X_res$actuals
Y_res$residuals = Y_res$preds - Y_res$actuals

X_RSS <- c(crossprod(X_res$residuals))
X_MSE <- X_RSS / length(X_res$residuals)
X_RMSE <- sqrt(X_MSE)

Y_RSS <- c(crossprod(Y_res$residuals))
Y_MSE <- Y_RSS / length(Y_res$residuals)
Y_RMSE <- sqrt(Y_MSE)

comboRMSE <- sqrt(X_MSE + Y_MSE)
```

# Results

As shown in Fig. 1 and Fig. 2, we found found that \texttt{k} of 4 minimized the error for X and \texttt{k}
of 5 minimized the error for Y. Fig. 3 visualizes the RMSE of the combination of X and Y models.
Based the CV results, 3 was chosen for the value of \texttt{k} for the position X model and 4 was chosen for
the value of \texttt{k} for the position Y model.

**Internal Cross-Validation Model Performance**

| AP Feature Set         | Root Mean Squared Error: Y  | Root Mean Squared Error: X  | 
|------------------------|-----------------------------|-----------------------------|
| Excluding \texttt{00:0f:a3:39:dd:cd}  | 1.632                 | 1.783
| Excluding \texttt{00:0f:a3:39:e1:c0}  | 1.634                 | 1.714
| All APs                | 1.622                 | 1.615

\newpage
```{r RMSE X Plots, echo=FALSE}
errorsX <- data.frame(
   k = rep(model.all.y$results$k, 3),
   model = c(
      rep('All APs', length(model.all.x$results$k)),
      rep('AP-c0', length(model.all.x$results$k)),
      rep('AP-cd', length(model.all.x$results$k))),
   errorX = c(model.all.x$results$RMSE,
                 model.c0.x$results$RMSE,
                 model.cd.x$results$RMSE)
)

errorsX %>%
   ggplot(aes(x = k, y = errorX, color = model)) +
   geom_line() + 
   xlab('k Neighbors') +
   ylab('RMSE') +
   ggtitle('RMSE of Position X Models with 5-Fold CV over k') +
   labs(caption = paste(
      'Figure 1:\n',
      'The mean RMSE of 5-fold cross validation for k-NN models using all APs (All APs),\n',
      'all APs excluding 00:0f:a3:39:dd:cd (AP-c0),',
      'and all APs excluding 00:0f:a3:39:e1:c0 (AP-cd)\nfor k values 1 through 15. This includes position X values only.', sep='')
      ) +
  theme_minimal() +
   theme(
      plot.title = element_text(hjust = 0.5),
      plot.caption = element_text(hjust = 0)
  )

```

```{r RMSE Y Plots, echo=FALSE}
errorsY <- data.frame(
   k = rep(model.all.y$results$k, 3),
   model = c(
      rep('All APs', length(model.all.y$results$k)),
      rep('AP-c0', length(model.all.y$results$k)),
      rep('AP-cd', length(model.all.y$results$k))),
   errorY = c(model.all.y$results$RMSE,
                 model.c0.y$results$RMSE,
                 model.cd.y$results$RMSE)
)

errorsY %>%
   ggplot(aes(x = k, y = errorY, color = model)) +
   geom_line() + 
   xlab('k Neighbors') +
   ylab('RMSE') +
   ggtitle('RMSE of Position Y Models with 5-Fold CV over k') +
   labs(caption = paste(
      'Figure 1:\n',
      'The mean RMSE of 5-fold cross validation for k-NN models using all APs (All APs),\n',
      'all APs excluding 00:0f:a3:39:dd:cd (AP-c0),',
      'and all APs excluding 00:0f:a3:39:e1:c0 (AP-cd)\nfor k values 1 through 15. This includes position Y values only.', sep='')
      ) +
  theme_minimal() +
   theme(
      plot.title = element_text(hjust = 0.5),
      plot.caption = element_text(hjust = 0)
  )

```





```{r RMSE All Plots, echo=FALSE}
errorsALL <- data.frame(
   k = rep(model.all.y$results$k, 3),
   model = c(
      rep('All APs', length(model.all.y$results$k)),
      rep('AP-c0', length(model.all.y$results$k)),
      rep('AP-cd', length(model.all.y$results$k))),
   errorALL = c(model.all.y$results$RMSE + model.all.x$results$RMSE,
                 model.c0.y$results$RMSE + model.c0.x$results$RMSE,
                 model.cd.y$results$RMSE + model.cd.x$results$RMSE)
)

errorsALL %>%
   ggplot(aes(x = k, y = errorALL, color = model)) +
   geom_line() + 
   xlab('k Neighbors') +
   ylab('RMSE') +
   ggtitle('RMSE of Models of both Positions (X,Y) with 5-Fold CV over k') +
   labs(caption = paste(
      'Figure 1:\n',
      'The mean RMSE of 5-fold cross validation for k-NN models using all APs (All APs),\n',
      'all APs excluding 00:0f:a3:39:dd:cd (AP-c0),',
      'and all APs excluding 00:0f:a3:39:e1:c0 (AP-cd)\nfor k values 1 through 15.', sep='')
      ) +
  theme_minimal() +
   theme(
      plot.title = element_text(hjust = 0.5),
      plot.caption = element_text(hjust = 0)
  )

```



### Simulated Online Model Error
The estimated Root Mean Square Error for the ensemble model applied to the online data set is roughly 7.638. We derived
this value by taking the mean squared error for the residual values on the holdout sets for both the predicted X
positions and predicted Y positions. The model estimating positions Y outperformed the model estimated positions X by
RMSE of 1.823 to 7.418, respectively.

| Positional Predictions           | Root Mean Squared Error | 
|----------------------------------|-------------------------|
| Predicted X                      | 6.732                   |
| Predicted Y                      | 2.067                   |
| Online X-Y Ensemble Predictions  | 7.042                   |


# Conclusion

KNN is a slow-learning algorithm because it computes distance across the entire data set every time it is executed, as
noted earlier. The calculation scales as shown in (\ref{eqn:knncomplexity}). Using the data we trained our models on,
the results were not very stable. Furthermore, calculation time is not practical for a real-time location as the overhead
time consumption for computation is too great. Visualized in the plot of actuals and predicted values in Fig. 4, below,
there are extreme outliers beyond the boundaries of the building walls. The high volume of extreme values is not
practical.

```{r GGPLOT for PREDS, echo=FALSE}
preds_x<- predict(model.all.x, online_pivot[,c(6,8:14)])
preds_y<- predict(model.all.y, online_pivot[,c(6,8:14)])

data.frame(
  x=c(preds_x, online_pivot$posX),
  y=c(preds_y, online_pivot$posY),
  group=c(rep('Prediction', length(preds_y)), rep('Actual', length(online_pivot$posY)))
) %>%
  ggplot(aes(x=x, y=y, color = group)) + geom_point(aes(shape=group)) +
   ggtitle('K-Nearest Neighbors: Predictions & Actuals') +
  ylab("Position Y") +
  xlab("Position X") +
   labs(caption = paste(
      'Figure 4:',
      'Positions of predicted X-Y values (blue) and actual values (red)')
      ) +
  theme_minimal() +
  theme(
      plot.title = element_text(hjust = 0.5),
      plot.caption = element_text(hjust = 0.5)
  ) + scale_color_manual(values=c("blue4", "coral2"))
tinytex::tlmgr_update()

```

Given that electromagnetic signal strength decreases by $1/r^2$, per unit distance, polynomial regression may be a highly
suited alternative to the K-Nearest Neighbors model used. Polynomial regression has a much smaller time complexity than
 K-NNsince it does not have to re-evaluate the entire data set with each added observation.

The following assumptions would need to be met for a regression approach to be efficable:

* Variance of the residuals is constant
* Normally distributed residuals
* No outliers with high leverage

\newpage

\appendix

# Code

The code used to produce the analysis in shown below.

```

#' Process a single line of the data files.
#'
#' @description
#' Produces a list of matricies from a data line.
#' The columns are as follows:
#' time, scanMAC, positionX, positionY, positionZ,
#' orientation, MAC, signalRSSI, channel, router_type
#'
#' @param x A file line
#'
processLine = function(x)
{
  # tokenize line on delimiters ;=,
  tokens = strsplit(x, "[;=,]")[[1]]
  # return null when there are no measurements
  if (length(tokens) == 10) 
    return(NULL)
  # get matrix of measured RSSI
  tmp = matrix(tokens[ - (1:10) ], , 4, byrow = TRUE)
  # add handheld device data and return resulting matrix
  cbind(matrix(tokens[c(2, 4, 6:8, 10)], nrow(tmp), 6, 
               byrow = TRUE), tmp)
}

#' Round the measurement angle to the nearest 45 deg.
#'
#' @param angles a vector of angles; expected 0 - 360
#'
roundOrientation = function(angles) 
  {
  # create a sequence of reference angles
  # 0, 45, 90, ... , 315
  refs = seq(0, by = 45, length  = 9)
  # round angles to the closest reference value
  q = sapply(angles, function(o) which.min(abs(o - refs)))
  c(refs[1:8], 0)[q]
}


#' Load data files for this case study.
#' 
#' @description
#' Produces a data.frame with the following columns
#' "time", "posX", "posY", "orientation", "mac", "signal",
#' "rawTime", "angle"
#' 
#' Only regular access points are kept (router_type==3).
#' Time is converted from milliseconds to seconds.
#' 
#'
#' @param filename The path to the data file
#' @param subMacs A vector of MAC addresses to use in the data
#'
readData = 
  function(filename = './offline.data.txt', 
           subMacs = c("00:0f:a3:39:e1:c0", "00:0f:a3:39:dd:cd", "00:14:bf:b1:97:8a",
                       "00:14:bf:3b:c7:c6", "00:14:bf:b1:97:90", "00:14:bf:b1:97:8d",
                       "00:14:bf:b1:97:81"))
  {
    txt <- readLines(filename)
    lines <- txt[ substr(txt, 1, 1) != "#" ]
    tmp <- lapply(lines, processLine)
    offline <- as.data.frame(do.call("rbind", tmp), 
                             stringsAsFactors= FALSE) 
    
    names(offline) <- c("time", "scanMac", 
                        "posX", "posY", "posZ", "orientation", 
                        "mac", "signal", "channel", "type")
    
    # keep only signals from access points
    offline <- offline[ offline$type == "3", ]
    
    # drop scanMac, posZ, channel, and type - no info in them
    dropVars <- c("scanMac", "posZ", "channel", "type")
    offline <- offline[ , !( names(offline) %in% dropVars ) ]
    
    # drop more unwanted access points
    offline <- offline[ offline$mac %in% subMacs, ]
    
    # convert numeric values
    numVars <- c("time", "posX", "posY", "orientation", "signal")
    offline[ numVars ] <- lapply(offline[ numVars ], as.numeric)
    
    # convert time to POSIX
    offline$rawTime <- offline$time
    offline$time <- offline$time/1000
    class(offline$time) <- c("POSIXt", "POSIXct")
    
    # round orientations to nearest 45
    offline$angle = roundOrientation(offline$orientation)
    
    return(offline)
  }

```


