{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "by Stuart Miller, Paul Adams, and Justin Howard\n",
    "\n",
    "# **Introduction**\n",
    "\n",
    "Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Methods**\n",
    "\n",
    "## **Data**\n",
    "\n",
    "Banking data, we don't know anything about the data!\n",
    "\n",
    "## **Models**\n",
    "\n",
    "This case study we use random forest, XGBoost, and support vector machines to model the data.\n",
    "In the following sections, we descibe how these models work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Forest**\n",
    "\n",
    "A random forest is an emsemble model created from a collection of decision trees and bootstrapped aggregated (bagged) data.\n",
    "The following steps are used to create bagged trees (James et al, 2013):\n",
    "\n",
    "  * bootstrap sample (repeated sampling with replacement) the dataset to create $B$ separate datasets.\n",
    "  * fit a model $f^b(x)$ on each $B$ dataset.\n",
    "\n",
    "Then the bagged model is given by\n",
    "\n",
    "$$\n",
    "f_{bag}(x) = \\frac{1}{B} \\sum_{b=1}^B f^b (x)\n",
    "$$\n",
    "\n",
    "In the context of classification, the *majority vote* of the classifiers is taken as the class prediction.\n",
    "This is called a bagged decision tree model.\n",
    "The aggregation of these high variance decision trees substantially reduces the overall model variance (James et al, 2013).\n",
    "In general, a large number of decision tree used be used in the ensemble.\n",
    "We treat the number of decision trees as a hyperparameter and tune it with cross-validation.\n",
    "\n",
    "One additional tweek is added to a bagged decision tree to make it a random forest.\n",
    "When building decision trees, at each split, the decision tree only considers a random subset of the available predictors (James et al, 2013).\n",
    "This improves on the bagged model by decorrelating the individual trees used in the ensemble.\n",
    "Two common methods for determing the number of predictors to consider in a split are the square root of the number of available features\n",
    " and log base-2 of the number of available features.\n",
    "We treat the number of features the model considers at each split as a hyperparameter and tune it with cross-validation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **XGBoost**\n",
    "\n",
    "[XGB](https://towardsdatascience.com/from-zero-to-hero-in-xgboost-tuning-e48b59bfaf58)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SVM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hyperparameter Tuning**\n",
    "\n",
    "Hyperparameters were selected with a randomized search with 5-fold internal cross-validation.\n",
    "We used a randomized search rather than an exhaustive search (sometimes called grid search) \n",
    " because randomized searches have been shown to achieve similar results,\n",
    " but with significantly lower run times than exhaustive search.\n",
    "Unlike in an exhaustive search where all possible combinations of tuning parameters are validated,\n",
    " in a randomized search a number of search iterations are specified and\n",
    " a random set of parameters are validated on each iteration\n",
    "In this application of randomized search, 5-fold cross-validation is performed at each iteration.\n",
    "The best parameters from the search are selected based on the mean cross-validated log loss.\n",
    "\n",
    "We ran the random forest and XGBoost hyperparameter searches for 100 iterations.\n",
    "We only ran the SVM hyperparameter search for 10 iterations due to the high run time of fitting the model.\n",
    "The tuning parameters for each model used in the case study are shown in tables X-Z.\n",
    "\n",
    "\n",
    "**Table X. Random Forest Tuning Parameters**\n",
    "\n",
    "| Parameter           | Search Range                    | Description |\n",
    "|---------------------|:-------------------------------:|---------------------|\n",
    "| `n_estimators`      | 10:150                          | Number of decision trees to use in the random forest |\n",
    "| `criterion`         | One of `'gini', 'entropy'`      | The method for determining best split in decision trees |\n",
    "| `max_depth`         | 10:100                          | The maximum depth decision trees can grow |\n",
    "| `min_samples_split` | 2:100                           | The minimum number of samples required to make a split |\n",
    "| `min_samples_leaf`  | 2:100                           | The minimum number of sample required to make a leaf node |\n",
    "| `max_features`      | One of `'auto', 'sqrt', 'log2'` | The maximum number of features considered when making a split in a tree |\n",
    "\n",
    "**Table Y. XGBoost Tuning Parameters**\n",
    "\n",
    "| Parameter           | Search Range                    | Description |\n",
    "|---------------------|:-------------------------------:|---------------------|\n",
    "| `A`      |                        |  |\n",
    "| `B`         | One of       |  |\n",
    "\n",
    "\n",
    "**Table Z. SVM Tuning Parameters**\n",
    "\n",
    "| Parameter | Search Range                                | Description |\n",
    "|-----------|:-------------------------------------------:|---------------------|\n",
    "| `C`       | 0.001:10\\*                                  |  |\n",
    "| `kernel`  | One of `'linear', 'poly', 'rbf', 'sigmoid'` |  |\n",
    "| `gamma`   | One of `'scale', 'auto'`                    |  |\n",
    "\n",
    "\\*value distibution on a log scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Results**\n",
    "\n",
    "* Hyperparameter tuning tables\n",
    "* Validation results for RF, XGB, SVM <- let's do a test set here\n",
    "* Scaling times for SVM (1k, 2k, 5k, 10k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conclusion**\n",
    "\n",
    "the conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "James, G., Witten, D., Hastie, T., Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R . Springer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    log_loss,\n",
    "    accuracy_score,\n",
    "    make_scorer)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    RandomizedSearchCV, \n",
    "    cross_validate)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_loss_scorer = make_scorer(log_loss, greater_is_better=False)\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "random_state = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# get the data\n",
    "data = pd.read_csv('./data/case_8.csv')\n",
    "# put the target in another variable\n",
    "target = data.target\n",
    "# drop off ID and target\n",
    "data = data.drop(['ID', 'target'], axis=1)\n",
    "# get train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,\n",
    "                                                    target,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stuart/anaconda3/envs/tf2/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/stuart/anaconda3/envs/tf2/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "obj_columns = list(data.select_dtypes(include='object'))\n",
    "obj_col_encoders = {col: LabelEncoder() for col in obj_columns}\n",
    "\n",
    "for col in obj_col_encoders.keys():\n",
    "    obj_col_encoders[col].fit(data[col])\n",
    "    \n",
    "for col in obj_col_encoders.keys():\n",
    "    X_train[col] = obj_col_encoders[col].transform(X_train[col])\n",
    "    X_test[col] = obj_col_encoders[col].transform(X_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized CV search done. 100 iterations took 03::53::19\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=random_state)\n",
    "rf_params = {\n",
    "    'n_estimators': np.linspace(10, 150, dtype='int'),\n",
    "    'criterion':['gini', 'entropy'],\n",
    "    'max_depth': np.linspace(10, 100, dtype='int'),\n",
    "    'min_samples_split': np.linspace(2, 100, 50, dtype='int'),\n",
    "    'min_samples_leaf': np.linspace(2, 100, 50, dtype='int'),\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "search_iters = 100\n",
    "\n",
    "rf_RSCV_start_time = time.time()\n",
    "# setup search\n",
    "rf_RSCV = RandomizedSearchCV(rf_clf, rf_params, scoring=log_loss_scorer,\n",
    "                                 n_iter=search_iters, random_state=random_state)\n",
    "# seach\n",
    "rf_RSCV.fit(X_train, y_train)\n",
    "\n",
    "rf_RSCV_end_time = time.time()\n",
    "duration = rf_RSCV_end_time-rf_RSCV_start_time\n",
    "\n",
    "print(f'Randomized CV search done. {search_iters} iterations took \\\n",
    "{int(duration // 3600):02d}::{int((duration % 3600)//60):02d}::{int((duration % 3600) % 60):02d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini',\n",
      " 'max_depth': 83,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 48,\n",
      " 'n_estimators': 132}\n"
     ]
    }
   ],
   "source": [
    "# print the best parameters chosen by CV\n",
    "pprint.pprint(rf_RSCV.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get CV results with best parameters\n",
    "rf_clf.set_params(**rf_RSCV.best_params_)\n",
    "rf_cv = cross_validate(rf_clf, X_train, y_train, \n",
    "                       scoring={\n",
    "                           'log_loss':log_loss_scorer,\n",
    "                           'accuracy':accuracy_scorer\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF 5-fold Validation Performance\n",
      "Mean Log Loss\t7.697942288341889\n",
      "Mean Accuracy\t0.777126444284875\n"
     ]
    }
   ],
   "source": [
    "print('RF 5-fold Validation Performance')\n",
    "# note test_log_loss is negated due to how scorers work \n",
    "# in parameter searches in sklearn\n",
    "print('Mean Log Loss\\t{}'.format(np.mean(-rf_cv['test_log_loss'])))\n",
    "print('Mean Accuracy\\t{}'.format(np.mean(rf_cv['test_accuracy'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Test Set Performance\n",
      "Test Log Loss\t7.640917295253621\n",
      "Test Accuracy\t0.7787732598208132\n"
     ]
    }
   ],
   "source": [
    "# get performance on test set\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_y_test_pred = rf_clf.predict(X_test)\n",
    "\n",
    "print('RF Test Set Performance')\n",
    "print('Test Log Loss\\t{}'.format(log_loss(rf_y_test_pred, y_test)))\n",
    "print('Test Accuracy\\t{}'.format(accuracy_score(rf_y_test_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF fit on 1000 records took 0.4633328914642334\n",
      "RF fit on 2000 records took 0.9529027938842773\n",
      "RF fit on 5000 records took 2.6475236415863037\n",
      "RF fit on 10000 records took 5.744798898696899\n"
     ]
    }
   ],
   "source": [
    "# do some fit times for comparison with the SVM\n",
    "for size in [1000, 2000, 5000, 10000]:\n",
    "    sample = random_state.choice(np.arange(len(X_train)), size=size, replace=False)\n",
    "    X_train_sub = X_train.iloc[sample, :]\n",
    "    y_train_sub = y_train.iloc[sample]\n",
    "    start_time = time.time()\n",
    "    rf_clf.fit(X_train_sub, y_train_sub)\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f'RF fit on {size} records took {duration}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
