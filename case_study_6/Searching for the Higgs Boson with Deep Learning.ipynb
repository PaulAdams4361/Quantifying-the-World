{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\begin{center} Paul Adams, Stuart Miller, and Justin Howard \\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The dataset used in this case study is a set of Monte Carlo simulations of signals for process that produce Higgs bosons and background processes that do not produce Higgs bosons\\footnotemark\\footnotetext{https://archive.ics.uci.edu/ml/datasets/HIGGS}.\n",
    "The dataset contains 11 million instances of 28 features: 21 kenimatic properties measured by particle deterors in the accelerator and 7 engineered features.\n",
    "The target variable is a binary indicator where 1 indicates a Higgs processes and 0 indicates a background process.\n",
    "The reference paper indicates that the last 500,000 instances in this training set were used for model validation.\n",
    "We maintained this train-validation split in this case study, using the last 500,000 instances for validation and the prior instances for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replication of Model\n",
    "\n",
    "In this case study, we replicated the modeling performed by Baldi, Sadowski, & Whiteson on the Higgs boson dataset with deep neural networks (DNN).\n",
    "The model used in the reference study was a 5-layer multi-perceptron (MLP) with `tanh` activation, weight decay ($L2$ reularization) coefficient of $1 x 10^{5}$, and layers initialized with weights from the random normal. \n",
    "These hyperparamters are summerized in table 1.\n",
    "\n",
    "**Table 1. Model Hyperparameters**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\begin{center}\n",
    " \\begin{tabular}{l l } \n",
    " \\hline\n",
    " Hyperparameter & Value \\\\ \n",
    " \\hline\n",
    " Layers                      & $5$   \\\\ \n",
    " Weight Decay                & $1 x 10^{5}$   \\\\ \n",
    " Activation                  & \\texttt{tanh}   \\\\\n",
    " First Layer Initialization  & Random Normal ($\\mu = 0$, $\\sigma = 0.1$)   \\\\\n",
    " Middle Layer Initialization & Random Normal ($\\mu = 0$, $\\sigma = 0.05$)   \\\\\n",
    " Last Layer Initialization   & Random Normal ($\\mu = 0$, $\\sigma = 0.001$)   \\\\\n",
    " \\hline\n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\n",
    "\\vspace{\\baselineskip}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In addition to the model architecture, we also replicated the training process.\n",
    "The model was trainined with stochastic gradient descent (SGD) with a batch size of 100.\n",
    "The learning rate was initialized at 0.05 and decreased by a factor of 1.0000002 on each batch to a minumum rate of $1 x 10^{-6}$.\n",
    "The momentum was initialized to 0.9 and increated linearly to 0.99 over 200 epochs, remaining constant after the 200th epoch.\n",
    "For the stopping criterion, the reference paper indicates that early stopping with minimum change in error of 0.00001 over 10 epochs was used to determine when to stop the training process (resulting in training the model over 200-1000 epochs).\n",
    "However, the reference paper does not indicate what error metric was monitored for early stopping.\n",
    "We moitored the validation binary cross-entropy loss for early stopping as is typical practice in deep learning.\n",
    "The training process is summerized in table 2.\n",
    "In this study, the model was implemented with `TensorFlow`\\footnotemark\\footnotetext{https://pypi.org/project/tensorflow/2.2.0/} (version 2.2.0) because the framework used in the reference paper, `PyLearn2`\\footnotemark\\footnotetext{http://deeplearning.net/software/pylearn2/}, is no long actively maintained.\n",
    "\n",
    "\n",
    "**Table 2. Model Training Parameters**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\begin{center}\n",
    " \\begin{tabular}{l l } \n",
    " \\hline\n",
    " Training Method & Value \\\\ \n",
    " \\hline\n",
    " Optimizer & SGD   \\\\ \n",
    " Learning Rate & $0.05$ to $1 x 10^{-6}$ decreased on each batch by a factor of $1.0000002$   \\\\ \n",
    " Momentum & $0.9$ to $0.99$ linearly increased over $200$ epochs   \\\\\n",
    " Early Stopping & Minimum decrease of $0.00001$ validation loss over 10 epochs   \\\\\n",
    " \\hline\n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\n",
    "\\vspace{\\baselineskip}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggestions for Model Improvement\n",
    "\n",
    "Include in your report:\n",
    "Based on the class notes and discussion suggest improvements to the procedure. What are standard practices now versus when this paper was written? What kind of improvements do they provide?\n",
    "\n",
    "* dropout\n",
    "* ResNets (skip connections)\n",
    "* optimizers: Adam, RMSprop\n",
    "* layer initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "How would you quantify if your result duplicated the paper’s? **-> one sample t-test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* \\[1\\] Baldi, P., P. Sadowski, and D. Whiteson. “Searching for Exotic Particles in High-energy Physics with Deep Learning.” Nature Communications 5 (July 2, 2014). [https://arxiv.org/pdf/1402.4735.pdf](https://arxiv.org/pdf/1402.4735.pdf)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "\n",
    "\\appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2\n",
    "print(tf.__version__)\n",
    "\n",
    "auc_score = tf.keras.metrics.AUC()\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/HIGGS.csv', header = None)\n",
    "# from the paper: The last 500,000 examples are used as a test set.\n",
    "# The first column is the class label (1 for signal, 0 for background), followed by the 28 features\n",
    "train_test_split = data.shape[0] - 500000\n",
    "X_test = data.iloc[ train_test_split : , 1: ]\n",
    "y_test = data.iloc[ train_test_split : , 0 ]\n",
    "X_train = data.iloc[ : train_test_split , 1: ]\n",
    "y_train = data.iloc[ : train_test_split , 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def create_model(hidden_size,\n",
    "                 first_layer_init,\n",
    "                 hidden_layer_init,\n",
    "                 output_layer_init,\n",
    "                 weight_decay,\n",
    "                 starting_lr,\n",
    "                 metrics,\n",
    "                 ):\n",
    "    \"\"\"Create the model used in this case study\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        layers.Dense(hidden_size, activation='tanh',\n",
    "                    kernel_initializer=first_layer_init,\n",
    "                    kernel_regularizer=l2(weight_decay)\n",
    "                    ),\n",
    "        layers.Dense(hidden_size, activation='tanh',\n",
    "                    kernel_initializer=hidden_layer_init,\n",
    "                    kernel_regularizer=l2(weight_decay)\n",
    "                    ),\n",
    "        layers.Dense(hidden_size, activation='tanh',\n",
    "                    kernel_initializer=hidden_layer_init,\n",
    "                    kernel_regularizer=l2(weight_decay)\n",
    "                    ),\n",
    "        layers.Dense(hidden_size, activation='tanh',\n",
    "                    kernel_initializer=hidden_layer_init,\n",
    "                    kernel_regularizer=l2(weight_decay)\n",
    "                    ),\n",
    "        layers.Dense(hidden_size, activation='tanh',\n",
    "                    kernel_initializer=hidden_layer_init,\n",
    "                    kernel_regularizer=l2(weight_decay)\n",
    "                    ),\n",
    "        layers.Dense(1, activation='sigmoid',\n",
    "                    kernel_initializer=output_layer_init,\n",
    "                    kernel_regularizer=l2(weight_decay)\n",
    "                    )\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=optimizers.SGD(lr=starting_lr),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def plot_training_curves(history, title=None, save_path=None):\n",
    "    ''' Plot the training curves for loss and accuracy given a model history\n",
    "    '''\n",
    "    # find the minimum loss epoch\n",
    "    minimum = np.min(history.history['val_loss'])\n",
    "    min_loc = np.where(minimum == history.history['val_loss'])[0]\n",
    "    # get the vline y-min and y-max\n",
    "    loss_min, loss_max = (min(history.history['val_loss'] + history.history['loss']),\n",
    "                          max(history.history['val_loss'] + history.history['loss']))\n",
    "    acc_min, acc_max = (min(history.history['val_accuracy'] + history.history['accuracy']),\n",
    "                        max(history.history['val_accuracy'] + history.history['accuracy']))\n",
    "    # create figure\n",
    "    fig, ax = plt.subplots(ncols=2, figsize = (15,7))\n",
    "    fig.suptitle(title)\n",
    "    index = np.arange(1, len(history.history['accuracy']) + 1)\n",
    "    # plot the loss and validation loss\n",
    "    ax[0].plot(index, history.history['loss'], label = 'loss')\n",
    "    ax[0].plot(index, history.history['val_loss'], label = 'val_loss')\n",
    "    ax[0].vlines(min_loc + 1, loss_min, loss_max, label = 'min_loss_location')\n",
    "    ax[0].set_title('Loss')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_xlabel('Epochs')\n",
    "    ax[0].legend()\n",
    "    # plot the accuracy and validation accuracy\n",
    "    ax[1].plot(index, history.history['accuracy'], label = 'accuracy')\n",
    "    ax[1].plot(index, history.history['val_accuracy'], label = 'val_accuracy')\n",
    "    ax[1].vlines(min_loc + 1, acc_min, acc_max, label = 'min_loss_location')\n",
    "    ax[1].set_title('Accuracy')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].set_xlabel('Epochs')\n",
    "    ax[1].legend()\n",
    "    plt.show()\n",
    "    if save_path is not None:\n",
    "        fig.savefig(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We selected a five-layer neural\n",
    "#network with 300 hidden units in each layer, a learning\n",
    "#rate of 0.05, and a weight decay coefficient of 1 × 10−5\n",
    "\n",
    "hidden_size = 300\n",
    "starting_lr = 0.05\n",
    "weight_decay = 1e-6\n",
    "\n",
    "#Hidden units all used the tanh activation function.\n",
    "#Weights were initialized from a normal distribution with\n",
    "#zero mean and standard deviation 0.1 in the first layer,\n",
    "#0.001 in the output layer, and 0.05 all other hidden layers. \n",
    "#Gradient computations were made on mini-batches\n",
    "#of size 100. \n",
    "\n",
    "first_layer_init = initializers.RandomNormal(\n",
    "    mean=0.0, stddev=0.1, seed=seed\n",
    ")\n",
    "hidden_layer_init = initializers.RandomNormal(\n",
    "    mean=0.0, stddev=0.05, seed=seed\n",
    ")\n",
    "output_layer_init = initializers.RandomNormal(\n",
    "    mean=0.0, stddev=0.001, seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(hidden_size,\n",
    "                     first_layer_init,\n",
    "                     hidden_layer_init,\n",
    "                     output_layer_init,\n",
    "                     weight_decay,\n",
    "                     starting_lr,\n",
    "                     metrics=['accuracy',\n",
    "                           auc_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The learning rate decayed by a factor\n",
    "#of 1.0000002 every batch update until it reached a minimum of 10−6\n",
    "class LRSchedule(callbacks.Callback):\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        current_lr = K.get_value(model.optimizer.lr)\n",
    "        if current_lr > 1e-6:\n",
    "            lr = current_lr / 1.0000002 #1.00002\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, 1e-6)\n",
    "\n",
    "lr_scheduler = LRSchedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A momentum term increased linearly over\n",
    "#the first 200 epochs from 0.9 to 0.99, at which point it\n",
    "#remained constant. \n",
    "\n",
    "class MomentumSchedule(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        starting_value = 0.9\n",
    "        ending_value = 0.99\n",
    "        number_epochs = 200\n",
    "        step_increase = (ending_value-starting_value) / number_epochs\n",
    "        if epoch > number_epochs:\n",
    "            K.set_value(self.model.optimizer.momentum, ending_value)\n",
    "        else:\n",
    "            current_momentum = K.get_value(self.model.optimizer.momentum)\n",
    "            current_momentum += step_increase\n",
    "            K.set_value(self.model.optimizer.momentum, step_increase)\n",
    "\n",
    "momentum_scheduler = MomentumSchedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training ended when the momentum had\n",
    "#reached its maximum value and the minimum error on\n",
    "#the validation set (500,000 examples) had not decreased\n",
    "#by more than a factor of 0.00001 over 10 epochs\n",
    "\n",
    "early_stopping_criterion = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.00001,\n",
    "    patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "105000/105000 [==============================] - 545s 5ms/step - loss: 0.5449 - accuracy: 0.7178 - auc: 0.7950 - val_loss: 0.5050 - val_accuracy: 0.7474 - val_auc: 0.8311\n",
      "Epoch 2/400\n",
      " 44271/105000 [===========>..................] - ETA: 5:01 - loss: 0.5009 - accuracy: 0.7505 - auc: 0.8329"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train,\n",
    "                 batch_size=100,\n",
    "                 epochs=400,\n",
    "                 validation_data=(X_test, y_test),\n",
    "                 callbacks=[\n",
    "                     lr_scheduler, momentum_scheduler, early_stopping_criterion\n",
    "                 ]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves(hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
