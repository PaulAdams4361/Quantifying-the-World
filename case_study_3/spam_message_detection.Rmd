---
title: "Spam Message Detection"
author: "Stuart Miller, Justin Howard, and Paul Adams"
date: October 6, 2020
lang: en-US
class: man
# figsintext: true
numbersections: true
encoding: UTF-8
# bibliography: references.bib
# biblio-style: apalike
output:
  bookdown::pdf_document2:
     citation_package: natbib
     keep_tex: false
     toc: false
header-includes:
   - \usepackage{amsmath}
   - \usepackage{gensymb}
   - \usepackage[utf8]{inputenc}
   - \usepackage[T1]{fontenc}
   - \usepackage{setspace}
   - \usepackage{hyperref}
   - \onehalfspacing
   - \newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
editor_options: 
  chunk_output_type: console
---

```{r}
install.packages("tm")
library("tm")
```


```{r, read_data, include=FALSE}
# video 1

# list the data diectories
list.dirs('./data/', full.names = F)

# get the number of data directories
length(list.files(paste('./data/', sep=.Platform$file.sep)))

# get all the data directory names
dir.names <- list.files(paste('./data', sep=.Platform$file.sep))

# list the number of files in each data directory
sapply(paste('./data', dir.names, sep = .Platform$file.sep), 
       function(dir) length(list.files(dir)))

# get the full data dir names
filldirNames <- paste('./data', dir.names, sep = .Platform$file.sep)

# list full dir names of data files; print sample
fileNames <- list.files(filldirNames[1], full.names = T)
fileNames[1]

# get some example enail files from the first dir
messageIdx <- c(1:5, 15, 27, 68, 69, 329, 404, 427, 516, 852, 971)
fn = list.files(filldirNames[1], full.names = T)[messageIdx]
sampleEmail <- sapply(fn, readLines)
sampleEmail
```


```{r, splitting, include=F}
# video 2

# get a sample email to work with
msg <- sampleEmail[[1]]

# get the blank line as a split point
which(msg == '')[1]
match('', msg)

# save the split point for example message
splitPoint <- match('', msg)

# view area around split point
msg[ (splitPoint - 2) : (splitPoint + 6) ]

# get the header and body of teh example message
header <- msg[1 : (splitPoint - 1)]
body <- msg[ -( 1:(splitPoint - 1) ) ]

# create a function to split messages on the first empty line
# header is the first element and body is the second element
# of the split
splitMessage <- function(msg){
  splitPoint <- match('', msg)
  header <- msg[1 : (splitPoint - 1)]
  body <- msg[ -( 1:(splitPoint - 1) ) ]
  return(list(header = header, body = body))
}

# apply function to the sample messages
sampleSplit <- lapply(sampleEmail, splitMessage)
length(sampleSplit)

# pring the sample header
sampleSplit[[6]]$header

```


```{r, removing_attachments, include=F}
# video 3

# get some files and split them as before
messageIdx <- c(1:5, 15, 27, 68, 69, 329, 404, 427, 516, 852, 971,
                1070, 1267, 1450, 1585, 1709, 1234, 1046, 1149,
                1641, 1406)
fn = list.files(filldirNames[1], full.names = T)[messageIdx]
sampleEmail <- sapply(fn, readLines)
sampleSplit <- lapply(sampleEmail, splitMessage)

header <- sampleSplit[[1]]$header
grep("Content-Type", header)

# find locations of "Contenct-Type" (where the attachment is)
headerList <- lapply(sampleSplit, function(msg) msg$header)
CTloc <- sapply(headerList, grep, pattern = "Content-Type")
CTloc

# improvement where NA is returned for 0 (no occurance)
sapply(headerList, function(header){
  CTloc <- grep("Content-Type", header)
  if (length(CTloc) == 0) return (NA)
  CTloc
})

# find locations of attachements
hasAttach <- sapply(headerList, function(header){
  CTloc <- grep("Content-Type", header)
  if (length(CTloc) == 0) return (FALSE)
  grepl('multi', tolower(header[CTloc]))
})


header <- sampleSplit[[12]]$header
boundaryIdx <- grep('boundary=', header)
header[boundaryIdx]

sub(".*boundary=\"? *(.*)\"?;?.*", "\\1", header[boundaryIdx])

# a function to find boundaries
getBoundry <- function(header){
  boundaryIdx <- grep('boundary=', header)
  boundary = gsub('"', "", header[boundaryIdx])
  gsub(".*boundary= *([^;]*);?.*", "\\1", boundary)
}

# sample message with a PGP signature
sampleSplit[[12]]$body

# search for boundary of msg 15
boundary = getBoundary(headerList[[15]]) 
body = sampleSplit[[15]]$body

# locating beginning string
bString = paste("--", boundary, sep = "")
bStringLocs = which(bString == body)
bStringLocs

# locating ending string
eString = paste("--", boundary, "--", sep = "")
eStringLoc = which(eString == body)
eStringLoc

# separating the body of the message
msg = body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1)]
tail(msg)

msg = c(msg, body[ (eStringLoc + 1) : length(body) ])
tail(msg)

# function that drops attachments
dropAttach = function(body, boundary){
  
  bString = paste("--", boundary, sep = "")
  bStringLocs = which(bString == body)
  
  if (length(bStringLocs) <= 1) return(body)
  
  eString = paste("--", boundary, "--", sep = "")
  eStringLoc = which(eString == body)
  if (length(eStringLoc) == 0) 
    return(body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1)])
  
  n = length(body)
  if (eStringLoc < n) 
     return( body[ c( (bStringLocs[1] + 1) : (bStringLocs[2] - 1), 
                    ( (eStringLoc + 1) : n )) ] )
  
  return( body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1) ])
}

head(sampleSplit[[1]]$body)


```


```{r Strip and Label, echo=T}
words = words[ !( words %in% stopWords) ]
words[0:15]

# Function to remove numbers, punctuation, and other undesirables
cleanText = function(msg){
  tolower(gsub("[[:punct:]0-9[:space:][:blank:]]+", " ", msg))
}

messageVerbiage = function(msg, stopWords){
 if(is.null(msg))
  return(character())

 words = unique(unlist(strsplit(cleanText(msg), "[[:blank:]\t]+")))
 
 # drop stopwords
 words = words[ nchar(words) > 1]
 words = words[ !( words %in% stopWords) ]
 invisible(words)
}

# Given directory and stop words, this extracts all email verbiage
getVerbiage = function(dirName, stopWords){
  # List all files in the directory
  fileNames = list.files(dirName, full.names = TRUE)
  
  # Remove all data except the emails
  notEmail = grep("cmds$", fileNames)
  if ( length(notEmail) > 0) fileNames = fileNames[ - notEmail ]

  messages = lapply(fileNames, readLines, encoding = "latin1")
  
  # break out the email message
  emailSplit = lapply(messages, splitMessage)

  bodyList = lapply(emailSplit, function(msg) msg$body)
  headerList = lapply(emailSplit, function(msg) msg$header)
  rm(emailSplit)
  
  # Email including attachment
  hasAttach = sapply(headerList, function(header) {
    CTloc = grep("Content-Type", header)
    if (length(CTloc) == 0) return(0)
    multi = grep("multi", tolower(header[CTloc])) 
    if (length(multi) == 0) return(0)
    multi
  })
  
  hasAttach = which(hasAttach > 0)
  
  # Get boundarys string in the emails
  boundaries = sapply(headerList[hasAttach], getBoundary)
  
  # Drop attachments from the email bodies
  bodyList[hasAttach] = mapply(dropAttach, bodyList[hasAttach], 
                               boundaries, SIMPLIFY = FALSE)
  
  # Get the message verbiage
  messageWordsList = lapply(bodyList, messageVerbiage, stopWords)
  
  invisible(messageWordsList)
}

massagesWordsList = lapply(filldirNames, getVerbiage, stopWords = stopWords) # All valid words from all the emails

numbaMassages = sapply(massagesWordsList, length)

# First three directories are emails and last two are spam. The five lists are unpacked into one.
isSpam = rep(c(FALSE, FALSE, FALSE, TRUE, TRUE), numbaMassages) # Identify the false messages

massagesWordsList = unlist(massagesWordsList, recursive = FALSE) # All valid words from all the emails
```


```{r Simple Bayes Classification, echo=T}

emailCnt = length(isSpam)
spamCnt = sum(isSpam)
legitCnt = emailCnt - spamCnt

set.seed(43)

### Test and train split

# Random-downsample to 25% without replacement
testSpamIdx = sample(spamCnt, size = floor(spamCnt*0.25))
testLegitIdx = sample(legitCnt, size = floor(legitCnt*0.25))

# split out into test and train data
testMessageWords = c((massagesWordsList[isSpam])[testSpamIdx],(massagesWordsList[!isSpam])[testHamIdx] )
trainMessageWords = c((massagesWordsList[isSpam])[ - testSpamIdx],(massagesWordsList[!isSpam])[ - testHamIdx])

testSpam = rep(c(TRUE, FALSE), c(length(testSpamIdx), length(testHamIdx)))
unique(testSpam)

trainSpam = rep(c(TRUE, FALSE), c(numSpam - length(testSpamIdx), numHam - length(testHamIdx)))
unique(trainSpam)
```

```{r Probability Estimation on Training Data, echo=T}
# number of unique words in training data
wordsUnique = unique(unlist(trainMessageWords))
length(wordsUnique)

# number of occurrences of each unique word in the spam message
spamWordCounts = rep(0, length(wordsUnique))
names(spamWordCounts) = wordsUnique

trainWords = lapply(trainMessageWords[trainSpam], unique)
trainTable = table(unlist(trainWords))
spamWordCounts[names(trainTable)] = trainTable

# number of occurrences of spam and non-spam for each word in the list.
spamVnotSpam = function(wordsList, spam, wordsUnique = unique(unlist(wordsList))){
  # table for spam, non-spam, and log odds
  wordTable = matrix(0.5, nrow = 4, ncol = length(wordsUnique), dimnames = list(c("spam", "non-spam", "presentLogOdds", "absentLogOdds"),  wordsUnique))

  # word count increases by one for each spam message there is
  spamWordCnt = table(unlist(lapply(wordsList[spam], unique)))
  wordTable["spam", names(spamWordCnt)] = spamWordCnt + 1

  # word count increases by one for each non-spam message there is
  legitWordCnt = table(unlist(lapply(wordsList[!spam], unique)))  
  wordTable["non-spam", names(legitWordCnt)] = legitWordCnt + 1

  # number of spam and non-spam
  spamCnt = sum(spam)
  hamCnt = length(spam) - spamCnt

  wordTable["spam",] = wordTable["spam",]/(spamCnt + .5) # probability of non-spam given spam
  wordTable["non-spam",] = wordTable["non-spam",]/(hamCnt + .5) # probability of spam given non-spam
  
  # log odds
  wordTable["presentLogOdds",] = log(wordTable["spam",]) - log(wordTable["non-spam", ])
  wordTable["absentLogOdds",] = log((1 - wordTable["spam",])) - log((1 -wordTable["non-spam",]))

  invisible(wordTable)
}



### Apply spamVnotSpam function to training data
trainTable = spamVnotSpam(trainMessageWords, trainSpam)
trainTable[,1:5]

```

```{r Classify Test Dataset, echo=T}
# There may be words not in training data. This calculates log odds without new words.
# The first message is spam, and the log likelihood is positive.
newMessage = testMessageWords[[1]]

newMessage = newMessage[!is.na(match(newMessage, colnames(trainTable)))]

present = colnames(trainTable) %in% newMessage

sum(trainTable["presentLogOdds", present]) + sum(trainTable["absentLogOdds", !present])

# A negative value with a large log likelihood ratio is in this non-spam message
newMessage = testMessageWords[[which(!testSpam)[1]]]
newMessage = newMessage[!is.na(match(newMessage, colnames(trainTable)))]
present = (colnames(trainTable) %in% newMessage)
sum(trainTable["presentLogOdds", present]) + sum(trainTable["absentLogOdds", !present])

# function to find log likelihood:
getLogLikelihood = function(words, freqTable){
  # remove words not in the training data
  words = words[!is.na(match(words, colnames(freqTable)))]

  # get words that DO appear in training data
  present = colnames(freqTable) %in% words

  sum(freqTable["presentLogOdds", present]) + sum(freqTable["absentLogOdds", !present])
}


# box plots of log likelihoods
testLogLikelihood = sapply(testMessageWords, getLogLikelihood, trainTable)
tapply(testLogLikelihood, testSpam, summary)

# generate the box plot
SPAM_CLASS = c("non-spam", "spam")[1 + testSpam]
boxplot(testLogLikelihood ~ SPAM_CLASS, ylab = "Log Likelihood Ratio Distribution", main = "Distribution of Randomly Selected Messages\n by  Classification", ylim=c(-500, 500))


# misclassification rate and using reference value tau
type_1_Error = function(tau, logLikelihoodVals, spam){
  classify = logLikelihoodVals > tau
  sum(classify & !spam)/sum(!spam)
}

# tau = 0
type_1_Error(0, testLogLikelihood,testSpam)

# tau = -20
type_1_Error(-20, testLogLikelihood,testSpam)

type_1_Error = function(logLikelihoodVals, isSpam){
  orderVals = order(logLikelihoodVals)
  logLikelihoodVals = logLikelihoodVals[orderVals]
  isSpam = isSpam[orderVals]

  idx = which(!isSpam)
  N = length(idx)
  list(error = (N:1)/N, values = logLikelihoodVals[idx])
}

type_2_Error = function(logLikelihoodVals, isSpam){
    
  orderVals = order(logLikelihoodVals)
  logLikelihoodVals =  logLikelihoodVals[orderVals]
  isSpam = isSpam[orderVals]

  idx = which(isSpam)
  N = length(idx)
  list(error = (1:(N))/N, values = logLikelihoodVals[idx])
}

# Get tau so one type of error is within 1% using 5-fold cross-validation.
k = 5
trainCnt = length(trainMessageWords)
partK = sample(trainCnt)
total = k * floor(trainCnt/k)
partK = matrix(partK[1:total], ncol = k)

testFoldOdds = NULL
for (i in 1:k){
  foldIdx = partK[,i]
  trainTabFold = spamVnotSpam(trainMessageWords[-foldIdx], trainSpam[-foldIdx])
  testFoldOdds = c(testFoldOdds, sapply(trainMessageWords[foldIdx], getLogLikelihood, trainTabFold))
}

testFoldSpam = NULL
for (i in 1:k){
  foldIdx = partK[,i]
  testFoldSpam = c(testFoldSpam, trainSpam[foldIdx])
}

xFoldI = type_1_Errors(testFoldOdds, testFoldSpam)
xFoldII = typeIIErrorRates(testFoldOdds, testFoldSpam)
tauFoldI = round(min(xFoldI$values[xFoldI$error <= 0.01]))
tauFoldI

tFold2 = xFoldII$error[ xFoldII$values < tauFoldI ]
max(tFold2)

```

































